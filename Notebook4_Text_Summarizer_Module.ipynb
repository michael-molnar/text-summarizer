{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "toxic-thompson",
   "metadata": {},
   "source": [
    "## AIDI 2004 - AI in Enterprise Systems\n",
    "\n",
    "### FINAL PROJECT\n",
    "\n",
    "by Michael Molnar and Vasundara Chandre Gowda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-memorabilia",
   "metadata": {},
   "source": [
    "Condensing long passages of text into short and representative snippets is a important task in natural language processing.  People can often be left frustrated at the end of reading an article, having found they had been fooled by a sensation, clickbait headline. \n",
    "\n",
    "In this project we will train and build our own deep learning model that will generate a novel summary that captures the main points of a news article, product review, or other text sample.    \n",
    "\n",
    "We will build a web application and deploy our model for use. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-deviation",
   "metadata": {},
   "source": [
    "### NOTEBOOK 4:\n",
    "\n",
    "In this notebook we will build the module that will be the basis of our Flask application.  This will take a user's input text, appropriately clean and process it, and then use our models to generate a summary of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "rocky-captain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "\n",
    "# Define the global variables\n",
    "MAX_LEN_TEXT = 50\n",
    "MAX_LEN_SUMMARY = 10\n",
    "\n",
    "# Load the tokenizers\n",
    "with open('text_tokenizer.pkl', 'rb') as handle:\n",
    "    text_tokenizer = pickle.load(handle)\n",
    "\n",
    "with open('summary_tokenizer.pkl', 'rb') as handle:\n",
    "    summary_tokenizer = pickle.load(handle)\n",
    "    \n",
    "# Create the mappings to go between integer indexes and words\n",
    "reverse_target_word_index = summary_tokenizer.index_word \n",
    "reverse_source_word_index = text_tokenizer.index_word \n",
    "target_word_index = summary_tokenizer.word_index\n",
    "\n",
    "# Import the models\n",
    "encoder_model = load_model('encoder_model.h5')\n",
    "decoder_model = load_model('decoder_model.h5')\n",
    "\n",
    "# Create the list of contraction mappings\n",
    "contraction_mappings = {\"ain't\": \"is not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"'cause\": \"because\", \n",
    "                       \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\", \n",
    "                       \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \n",
    "                       \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \n",
    "                       \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                       \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\n",
    "                       \"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \n",
    "                       \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \n",
    "                       \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \n",
    "                       \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \n",
    "                       \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\n",
    "                       \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \n",
    "                       \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
    "                       \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \n",
    "                       \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \n",
    "                       \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \n",
    "                       \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \n",
    "                       \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
    "                       \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \n",
    "                       \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \n",
    "                       \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \n",
    "                       \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \n",
    "                       \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n",
    "                       \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                       \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \n",
    "                       \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n",
    "                       \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \n",
    "                       \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \n",
    "                       \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n",
    "                       \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \n",
    "                       \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                       \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                       \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \n",
    "                       \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "# Import the list of stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Performs all necessary cleaning operations on text input.\n",
    "    \"\"\"\n",
    "    # Lowercase the text\n",
    "    new_text = text.lower()\n",
    "    # Remove special characters\n",
    "    new_text = re.sub(r'\\([^)]*\\)', '', new_text)\n",
    "    new_text = re.sub('\"', '', new_text)\n",
    "    # Expand contractions\n",
    "    new_text = ' '.join([contraction_mappings[x] if x in contraction_mappings else x for x in new_text.split(' ')])\n",
    "    # Remove 's \n",
    "    new_text = re.sub(r\"'s\\b\", '', new_text)\n",
    "    # Replace non-alphabetic characters with a space\n",
    "    new_text = re.sub('[^a-zA-Z]', ' ', new_text)\n",
    "    # Split the text into tokens and remove stopwords\n",
    "    tokens = [word for word in new_text.split() if word not in stopwords]\n",
    "    # Keep only tokens that are longer than one letter long \n",
    "    words = []\n",
    "    for t in tokens:\n",
    "        if len(t) > 1:\n",
    "            words.append(t)\n",
    "    # Return a rejoined string \n",
    "    return (' '.join(words).strip())\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    \"\"\"\n",
    "    Take the input sequence and use the models to generate the summary.\n",
    "    \"\"\"\n",
    "    # Encode the input as state vectors.\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "\n",
    "    # Chose the 'start' word as the first word of the target sequence\n",
    "    target_seq[0, 0] = target_word_index['start']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "\n",
    "        # Sample a token\n",
    "        \n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        \n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "\n",
    "        if(sampled_token!='end'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        # Exit condition: either hit max length or find stop word.\n",
    "        if (sampled_token == 'end' or len(decoded_sentence.split()) >= (MAX_LEN_SUMMARY-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update internal states\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "def summarize():\n",
    "    # Take input text\n",
    "    text = input('Enter your text: ')\n",
    "    # Clean the text\n",
    "    review = clean_text(text)\n",
    "    # Condense to the maximum lenght\n",
    "    condensed_review = review.split()[:MAX_LEN_TEXT]\n",
    "    # Join into a string \n",
    "    condensed_review = ' '.join(condensed_review)\n",
    "    # Tokenized\n",
    "    tokenized_review = text_tokenizer.texts_to_sequences([condensed_review])\n",
    "    # Pad the seuqence \n",
    "    pad_tokenized_review = pad_sequences(tokenized_review, maxlen = MAX_LEN_TEXT, padding = 'post')\n",
    "    # Generate the summary \n",
    "    summary = decode_sequence(pad_tokenized_review.reshape(1, MAX_LEN_TEXT))\n",
    "    # Print results\n",
    "    print('Summary: ', summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-february",
   "metadata": {},
   "source": [
    "### Generating Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "pleased-terrorism",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your text: give rating order make comment really like eden organic beans used many years usually purchased locally available however beans received online source date black beans expiration dates pinto beans expiration dates check dates open cooked one package beans thinking check date thought would even issue since ordering direct going mine old dates know use rest \n",
      "Summary:   great product poor packaging\n"
     ]
    }
   ],
   "source": [
    "summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "polar-final",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your text: grounds cup started packing much coffee could cups could barely get top made much better cup coffee however still leaked still grounds coffee make keurig run much slower mention cleanup business uses keurig brew coffee choice also couple tea hot chocolate drinkers everytime use disposable cups wipe machine drip tray cup also clean interior cup next person put leftovers nice idea amount work require makes paying little convenience regular cups attractive probably use back regular cups find landfill friendly solution \n",
      "Summary:   excellent coffee\n"
     ]
    }
   ],
   "source": [
    "summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "elder-reporter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your text: reviews decided purchase natural balance several different sites included natural balance among best dry food happy cat switched foods effortlessly even gradually introduce like one day eating purina one next natural balance one major thing noticed weeks food coat much softer shinier silky soft holy cow know good things fur good sign bowel movements always nice solid look even better food also feed less day makes bag last longer lbs purina one would convert keep buying food thanks natural balance \n",
      "Summary:   my cat loves this\n"
     ]
    }
   ],
   "source": [
    "summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "foster-microwave",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your text: problem though one store sells box seldom shelf store carries regularly sells box living rural area long ways nearest large town two groceries closest small town depend two stores getting frustrated either finding tea paying exorbitant price found delivered regular intervals shipping charges savings store prices ecstatic running store store hoping getting strange looks stand coffee tea aisle shelf empty wondering going drink feel good bigelow box empty favorite tea comes right mailbox reasonably priced automatically like magic life good \n",
      "Summary:   great price and taste\n"
     ]
    }
   ],
   "source": [
    "summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "sharing-criterion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your text: this is my favorite pizza\n",
      "Summary:   yummy\n"
     ]
    }
   ],
   "source": [
    "summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "technological-november",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your text: this is decent, though I have had better.\n",
      "Summary:   good\n"
     ]
    }
   ],
   "source": [
    "summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-accreditation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
